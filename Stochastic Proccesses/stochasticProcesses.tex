\documentclass[12pt]{book}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks=true,urlcolor=blue,backref,linkcolor=blue}
\usepackage{color}
\usepackage{float}

\begin{document}
\tableofcontents
\section{sigma-algebra, Probability space, and Random Variables}
From \cite{Oksendal14}. If $\Omega$ is a given set, a \textbf{$\sigma$-algebra} $\mathcal{F}$ on $\Omega$ is a family $\mathcal{F}$ of subsets of $\Omega$ with the following properties
\begin{enumerate}
\item $\emptyset \in \mathcal{F}$
\item for a set $F$, if $F\in \mathcal{F}$ then $F^C\in\mathcal{F}$, for $F^C= \Omega\backslash\mathcal{F}$
\item for sets $A_1,A_2,A_3,...\in\mathcal{F}\Rightarrow \cup_{i=1}^\infty A_i \in \mathcal{F}$
\end{enumerate}
Why is it important to have a $\sigma$-algebra in a probability space?
The $\sigma$-algebra represents all events that we can assign probabilities to. Property 1 together with property 2 states that in the space the probability of all events occurring, should be measurable and equal 1, whereas it's complement, no events occurred should be measurable and assigned probability 0. For other events, the probability of an event occurring and its complement should both be measurable. Property 3 assigns some "continuity" in the space of events and allows us to "group" elementary events without risking the creation of an event which is not included in the event space. 

The pair $(\Omega,\mathcal{F})$ is called a measurable space. A probability measure $P$ is a function $P:\mathcal{F}\rightarrow [0,1]$ which assigns each elementary event $A\in\mathcal{F}$ with a number representing its likelihood. 

The triplet $(\Omega,\mathcal{F},P)$ is a \textbf{probability space}. All probability spaces can be made complete, hence we refer to them as complete from now on. 

A function $Y:\Omega\rightarrow\mathbb{R}^n$ is said to be $\mathcal{F}$-measurable if
\begin{equation*}
Y^{-1}(U):=\{\omega\in \Omega ; Y(\omega)\in \mathcal{F}\}
\end{equation*}
For all open sets $U\in\mathbb{R}^n$. We call $\omega$ an elementary event. It is, in fact, a continuous trajectory in the space of continuous functions (but non-differentiable). 

A \textbf{random variable} $X$ is an $\mathcal{F}$-measurable function $X:\Omega\rightarrow \mathbb{R}^n$. Every random variable induces a probability measure $\mu_X$ on $\mathbb{R}^n$, defined by 

\begin{equation*}
\mu_X(B) = P(X^{-1}(B))
\end{equation*}

The function 
\begin{equation*}
(\omega,t)\rightarrow X(\omega,t)
\end{equation*}
represent a two-dimensional process, where fixing $t$ gives us all the values of the random process $X$ at a particular point in time (assuming $t$ represents time), whereas fixing $\omega$ gives us one trajectory over time (one realization of the process).The process $X$ is referred to as a \textbf{stochastic process}.


\section{The Space of Elementary Events}\label{section_theSpaceOfElementaryEvents}
From \cite{schuss2009theory}. The space of elementary events for a Brownian motion is the space of all continuous real functions 

\begin{equation*}
\Omega=\{\omega(t):\mathbb{R}_+\rightarrow\mathbb{R}\}
\end{equation*}


\section{Noise}[Unfinished]
White noise is a process for which there is no correlation between any values at different times. As an idealization of such a process there exist the \textbf{Orenstein-Uhlenbeck process} (to be defined) for which produces an almost uncorrelated noise. For this model, the second order correlation function can, up to a constant factor, be described as 
\begin{equation*}
<x(t),x(t')>= \frac{\gamma}{2}\exp(-\gamma|t-t'|)
\end{equation*}

\section{Cylinder Sets}
From \cite{schuss2009theory}.A cylinder set of Brownian trajectories is defined by a sequence of times $0\leq t_1<t_2<...<t_n$, and real intervals $I_k=(a_k,b_k)$, $k=1...n$, as 
\begin{equation*}
C(t_1,t_2,...,t_n;I_1,I_2,...I_n)=\{\omega\in \Omega|x(\omega,t_k)\in I_k\},\forall 1\leq k \leq n
\end{equation*}
For a cylinder not to contain a trajectory, it is enough that in one time point the trajectory is not in the interval.
The cylinder $C$ contains entire trajectory, which fulfill the inclusion demand. 

The joint PDF of $w(t_1,\omega),w(t_2,\omega),w(t_3,\omega)$ is the Weiner measure of the cylinder $C(t_1,t_2,t_3;I^x,I^y,I^z)$. These points belong to the same process $\omega$ at three different times. 

\section{Filtration}\label{section_filtration}
The $\sigma$-algebra $\mathcal{F}_t$ of Brownian event is defined by cylinder
sets confined to times $0\leq t_i < t$, for some fixed $t$. Obviously, $\mathcal{F}_s \subset \mathcal{F}_t \subset \mathcal{F}$ if $0\leq s < t <\infty$. The family of $\sigma$âˆ’ algebras $\mathcal{F}_t$ for $t \geq 0$ is called the Brownian filtration and is said to be generated by the Brownian events up to time $t$

In simple words, filtration is an increasing sequence of $\sigma$-algebras on a measurable space. 

\section{Adapted Process}\label{section_adaptedProcess}
The process $x(t,\omega)$ is said to be adapted
to the Brownian filtration $\mathcal{F}_t$ if $\{\omega\in\Omega|x(t,\omega) \leq y\} \in F_t$ for every $t \geq 0$ and
$y\in R$. In that case we also say that $x(t, \omega)$ is $\mathcal{F}_t$-measurable.

Thus an adapted process does not depend on the future behavior of the Brownian trajectory from time $t$ on.

\section{The Weiner Measure}\label{section_theWeinerMeasure}
The probability measure $Pr$ defined on all the Brownian trajectories (events in the set $\Omega$) are defined on cylinder sets and then extended to all events in the space by elementary properties of probability sets.

For a cylinder set $C(t;I)$, where $t>0$ and $I=(a,b)$, the Weiner measure is 
\begin{equation*}
Pr\{C(t;I)\}=\frac{1}{\sqrt{2\pi t}}\int_a^b\exp^{-x^2/2t}dx
\end{equation*}

\section{Martingale}\label{section_martingale}
A martingale is a stochastic process $ x(t)$ with $\mathbb{E}|x(t)|<\infty$ ,$\forall t$, and for $t_1<t_2<...<t_n$
\begin{equation*}
\mathbb{E}[x(t)|x(t_1)=x1,x(t_2)=x_2,...x(t_n)=x_n]=x_n
\end{equation*}

\section{Approximation to the stochastic path as $L^2[0,T]$ expansion}
From \cite{iacus2009simulation}. It $X(t)$ is a random trajectory of the process $X(\omega, t)$ for a given $\omega$, the Weiner process $W(t)$ has trajectories belonging to $L^2[0,T]$ for almost all $\omega$'s, and the \textbf{\href{http://en.wikipedia.org/wiki/Karhunen-Loeve_theorem}{Karhunen-Loeve}} expansion for it takes the form 

\begin{equation*}
W(t)=W(\omega,t)= \sum_{i=1}^\infty Z_i(\omega)\phi_i(t), 
\end{equation*}
with $0\leq t \leq T$, and 
\begin{equation*}
\phi_i(t) = \frac{2\sqrt{2T}}{(2i+1)\pi} \sin \left(\frac{(2i+1)\pi t}{2T} \right)
\end{equation*}

\section{Stochastic Integration}\label{section_stochasticIntegration}
For an arbitrary function of time $G(t')$ and a Weiner process $W(t)$, the stochastic integral 
\begin{equation*}
\int_{t_0}^t G(t')\mathrm{d}W(t')
\end{equation*}
is defined as a kind of Riemann integral, that is, we divide the interval $[t_0,t]$ into $n$ subintervals $t_0\leq t_1\leq ...\leq t$, and define intermediate points $t_{i-1}\leq\tau_i\leq t_{i}$. The integral is then defined as a limit of the partial sums 
\begin{equation*}
S_n=\sum_{i=1}^nG(\tau_i)[W(t_i)-W(t_{i-1})]
\end{equation*}
Note that the Weiner process in the partial sums is contributing the values at the edges of the interval, whereas the function $G$ contributes the values somewhere in its middle.
The choice of $\tau_i$ affects the value of the integral. It can be seen that f we choose $\tau_i=\alpha t_{i-1}+(1-\alpha)t_i$, with $0\leq \alpha\leq 1$,  than the mean value of the integral can take any value between 0 and $(t-t_0)$. 

for the \textbf{It\^{o} integral}, we choose $\tau_i=t_{i-1}$, which is the left boundary of the subinterval and thus define the stochastic integral as
\begin{equation*}
\int_{t_0}^{t}G(t')\mathrm{d}W(t')=ms-lim_{n\rightarrow\infty}\left\{\sum_{i=1}^nG(t_{i-1})[W(t_{i})-W(t_{i-1})] \right\}
\end{equation*}
by ms-lim, we mean the mean-square limit, which is defined by 
\begin{equation*}
ms-lim_{n\rightarrow\infty}\int p(\omega)[X_n(\omega)-X(\omega)]^2dw= lim_{n\rightarrow \infty}<(X_n-X)^2>
\end{equation*}

if indeed $lim_{n\rightarrow \infty}<(X_n-X)^2>$ we say that $ms-lim_{n\rightarrow \infty} X_n=X$.

One major consequence of the choice $\tau_i=t_{i-1}$ is that 
\begin{equation*}
\int_{t_0}^{t}W(t')dW(t')=0
\end{equation*}
That is, the stochastic integral of the Weiner process over any interval is zero.
Because the left point of the subinterval is chosen, the value of the function $G(t'_{i-1})$ is independent of the increments $[W(t_i)-W(t_{i-1})]$, since $G$ is $F_t$ adapted process.

\section{Properties of It\^{o} integral}\label{section_propertiesOfTheItoIntegral}
We define the class $H_2[0,T]$ of $F_t$-adapted stochastic processes such that 
\begin{equation*}
\int_0^T \mathbb{E}f^2(s,\omega)ds <\infty
\end{equation*}
and list the following properties
\begin{enumerate}
\item \textbf{linearity}. For $f(t),g(t)\in H_2[0,T]$, and $\alpha,\beta$ real, then $\alpha f(t)+\beta g(t) \in H_2[0,T]$, and 
\begin{equation*}
\int_0^t \alpha f(s)+\beta g(s) d\omega(s) = \alpha \int_0^t f(s)d\omega(s) +\beta \int_0^t g(s)d\omega(s)
\end{equation*}
\item \textbf{additivity}. If $f(t)\in H_2[0,T_1]$ and $f(t)\in H_2[T_1,T]$ for $0<T_1<T$, then 
\begin{equation*}
\int_0^Tf(s)d\omega(s) = \int_0^{T_1}f(s)d\omega(s) +\int_{T_1}^T f(s)d\omega(s)
\end{equation*}
\item for a deterministic and integrable $f(t)$
\begin{equation*}
\int_0^t f(s)d\omega(s) \sim N\left(0,\int_0^t f^2(s)ds\right)
\end{equation*}
\item for $f(t)\in H_2[0,T]$
\begin{equation*}
\mathbb{E}\int_0^t f(s)d\omega(s) = 0
\end{equation*}
\item for $0<\tau<t<T$  
\begin{equation*}
\mathbb{E}\left[\int_0^t f(s)d\omega(s)  | \int_0^\tau f(s)d\omega(s)=x \right] =x 
\end{equation*}
\item for $f(t),g(t)\in H_2[0,T]$
\begin{equation*}
\mathbb{E}\left[\int_0^Tf(s)d\omega(s)\int_0^T g(s)d\omega(s)\right]=\int_0^T\mathbb{E}[f(s)g(s)]ds
\end{equation*}
note that the right-hand-side is integrated with respect to $s$.
\end{enumerate}
We can see by property 5 that the It\^{o} integral is a martingale (see subsection \ref{martingale}).

\section{Integration with Respect to a Measure}
A measure $\mu_A$ defines an integral of a nonnegative measurable function $f(\omega)$ by 
\begin{equation*}
\int_{\Omega}f(\omega)d\mu(\omega)=\lim_{h\rightarrow 0}\lim_{N\rightarrow \infty}\sum_{n=0}^{N}nh\mu\{\omega: nh\leq f(\omega)\leq(n+1)h\}
\end{equation*}
If the limit exists we say the $f(\omega)$ is a measurable function. 


\section{The It\^{o} Calculus}\label{section_theItoCalculus}
From \cite{schuss2009theory} p.82. Consider two processes $a(t),b(t)$ of class $H_2[0,T]$ (defined in \ref{section_propertiesOfTheItoIntegral}) and define the stochastic process
\begin{equation*}
x(t) = x_0 + \int_0^t a(s)ds + \int_0^t b(s)d\omega(s)
\end{equation*}

where $x_0$ is a random variable independent on $\omega(t),\forall t>0$, then for $0\leq t_1\leq t_2 \leq T$
\begin{equation*}
x(t_2)-x(t_1)=\int_{t_1}^{t_2}a(s)ds +\int_{t_1}^{t_2}b(s)d\omega(s)
\end{equation*}
We abbreviate the this notation by 
\begin{equation*}
dx=a(t)dt+b(t)d\omega(t)
\end{equation*}

\textbf{Example 1:} We calculate the differential of $\omega^2(t)$. By the definition of the Ito integral we know that 
\begin{equation*}
\omega^2(t_2)-\omega^2(t_1)=(t_2-t_1)+2\int_{t_1}^{t_2}\omega(s)d\omega(s)=\int_{t_1}^{t_2}1dt+2\int_{t_1}^{t_2}\omega(s)d\omega(s)
\end{equation*}. 
In our abbreviation we can write 
\begin{equation*}
d\omega^2(t) = 1dt+2\omega(t)d\omega(t)
\end{equation*}
with $a(t)=1$ and $b(t)=2\omega(t)$. So the Ito calculus does not obey the normal chain rule of calculus.
$\partial R$
\textbf{Example 2:} We now calculate the differential of $f(t)\omega(t)$. If $f(t)$ is a deterministic smooth function then we can integrate by parts
\begin{equation*}
\int_{t_1}^{t_2}f(t)d\omega(t)= f(t_2)d\omega(t_2)-f(t_1)d\omega(t_1)-\int_{t_1}^{t_2}f'(t)\omega(t)dt
\end{equation*}
we now set $x(t)= f(t)\omega(t)$ so 
\begin{equation*}
dx(t) = f'(t)d\omega(t)dt+f(t)d\omega(t)=\omega(t)df(t)+f(t)d\omega(t)
\end{equation*}
as in the classical calculus. 

\textbf{Example 3:} the differential of the product of functions. If $x_1(t)$ and $x_2(t)$ have the Ito differentials
\begin{equation*}
dx_1dt = a_1(t)dt +b_1(t)d\omega(t)\\
\end{equation*}
\begin{equation*}
dx_2(t) =a_2(t)dt+b_2(t)d\omega(t) 
\end{equation*}
where $a_1,b_1,a_2,b_2\in H_2[0,T]$ (see \ref{propertiesOfTheItoIntegral}), then
\begin{equation*}
d[x_1(t)x_2(t)]= x_1(t)dx_2(t)+x_2(t)dx_1(t)+b_1(t)b_2(t)dt
\end{equation*}

\subsection{The Wong-Zakai Correction}\label{section_theWongZakaiCorrection}
From \cite{schuss2009theory}. To see the relationship between the Ito integral and the Stratonovich integral we have the Wong-Zakai correction.  If $f(x,t)$ has a second continuous derivative such that $|f_{xx}(x,t)|<A(t)e^{\alpha(t)|x|} $ for some positive continuous function $\alpha(t)$ and $A(t)$ for all $a\leq t \leq b$ then
\begin{equation}
\int_{a}^b f(\omega(t),t)d_S\omega(t) = \int_a^b f(\omega(t),t)d\omega(t)+\frac{1}{2}\int_a^b\frac{\partial}{\partial x}f(\omega(t),t)dt
\end{equation}
where $\omega_S(t)$ represents the Stratonovich integral.

\textbf{Example 1:} We now use the Wong-Zakai correction to show that the Stratnovich differential satisfies the classical rule
\begin{equation*}
d_Sx_1(t)x_2(t)= x_1(t)dx_2(t) +x_2(t)dx_1(t)
\end{equation*}

The Statonovich differential is defined as 
\begin{equation*}
d_Sx(t) = a(t)dt+b(t)d_S\omega(t)
\end{equation*}

\section{The Langevin Equation}\label{section_theLangevinEquation}
[Unfinished]
An equation of the type 
\begin{equation*}
\frac{dx}{dt}=a(x,t)+b(x,t)\xi(t)
\end{equation*}

\section{The It\^{o} Equation}\label{section_theItoEquation}
Equation of the form 
\begin{equation*}
 \mathrm{d} X_t = \mu(X_t,t)\, \mathrm{d} t +  \sigma(X_t,t)\, \mathrm{d} B_t
\end{equation*}
is an informal way of expressing the more appropriate integral equation 
\begin{equation*}
X_{t+s} - X_{t} = \int_t^{t+s} \mu(X_u,u) \mathrm{d} u + \int_t^{t+s} \sigma(X_u,u)\, \mathrm{d} B_u
\end{equation*}
where the dynamic of the particle $X$ is given as a sum of two integrals, the first one is a standard Lebesgue integral, and the second is an It\^{o} integral (to be defined).  
The functions $\mu(X_t,t)$ and $\sigma(X_t,t)$ are well defined continuous functions, and $B_t$ is a Brownian motion (a Weiner process, or white noise). An informal way of interpreting the equation above is to say that at each small time interval of size $\delta$ the process $X_t$ is changed by a normally distributed value with expectation $\mu(X_t,t)\delta$ and variance $\sigma(X_t,t)^2\delta$. 
the function $\mu(X_t,t)$ is referred to a s the drift coefficient, and the function $\sigma(X_t,t)$ as the diffusion coefficient. 

\section{Existence and Uniqueness of It\^{o} SDE solutions}
For an Ito SDE taking values in n-dimensional Euclidean space, if for $T>0$\\
\begin{eqnarray*}
&\mu:   &\mathbb{R}^2\times[0,T]\rightarrow \mathbb{R}^n\\
&\sigma:&\mathbb{R}^n \times[0,T] \rightarrow \mathbb{R}^{n\times m}
\end{eqnarray*}

are measurable functions, for which there exist constants $C$ and $D$ such that 
\begin{eqnarray*}
& &|\mu(x,t)|+|\sigma(x,t)|\leq C(1+|x|)\\
& &|\mu(x,t)-\mu(y,t)|+|\sigma(x,t)-\sigma(y,t)|\leq D|x-y|
\end{eqnarray*}
for all $t\in [0,T]$ and all $x,y\in \mathbb{R}^n$, and $|\sigma|^2=\sum_{i,j=1}|\sigma_{i,j}|^2$. 
If $Z$ is a random variable, independent of the $\sigma$-algebra generated by $B_s$, $s\geq 0$, and with finite second moment, then the SDE, with initial condition $X_0=Z$ has an almost surely unique solution in $t\in[0,T]$, $X_t(\omega)$, such that $X$ is adapted to the filtration $\mathcal{F}_t$ generated by $Z$ and $B_s$, $s\leq t$.

\section{Time Correlation Function and Response Function}\label{section_timeCorrelationAndResponseFunction}
Taken from Doi \& Edwards. One characterization of a Brownian path is the time correlation function. This function is defined as follows, for a quantity $A$ of a system of Brownian particles for many samples in the \textit{equilibrium} state. Let $A(t)$ be the measured values of $A$ at time $t$. The time correlation function $C_{AA}(t)$ is the average of $A(t)A(0)$
\begin{equation*}
C_{AA}(t)=\left<A(t)A(0) \right>
\end{equation*}
Averaging is performed over samples (not time). $C_{AA}(t)$ usually decreases with time since $A(t)$ and $A(0)$ become uncorrelated with time, and becomes $\left< A(t)\right> \left< A(0)\right>$. the time in which these quantities become uncorrelated is called the \textit{correlation time}. 

The time correlation is also defined for different quantities 
\begin{equation*}
C_{AB}(t)= \left< A(t)B(0) \right>
\end{equation*}
which is the cross correlation function. 
 
\section{Non-anticipating Functions}{[Unfinished]}
\section{Stochastic Differential Equations}
\section{First passage problems}\label{section_firstPassageProblems}
We first consider the discrete Markov process, described by the "forward" equation
\begin{equation*}
\frac{\partial P_{ki}}{\partial t}=M_{kj}P_{ji}
\end{equation*}
where $P_{kj}$ is the probability that the system is in state $k$ at time $t$ given that it started at state $i$ at $t=0$. The absorbing states, $\mathcal{A}$, of the system gives transition rates of zero in the transition matrix $M$. The \textit{survival probability}, defined as 
$S_{i}(t)=\sum_{k\notin\mathcal{A}}P_{ki}(t)$ will vanish as $t\rightarrow\infty$.

The \textit{first passage time distribution} can be derived from $S_i(t)$. For this end, it is convenient to consider the adjoint equation that is also obeyed by $P_{ki}(t)$. If the transition matrix $M_{kj}$ is time-independent, then the "backward" equation:
\begin{equation*}
\frac{\partial P_{ki}}{\partial t}=P_{kj}M_{ji}
\end{equation*}
This equation does not operate on the final state $k$, so we can calculate the sum $S_{i}(t)=\sum_{k\notin\mathcal{A}}P_{ki}(t)$ to find the survival probability 
\begin{equation*}
\frac{d S_i}{dt}=S_j(t)M_{ji}=-J_i(t)
\end{equation*}
The boundary conditions $S_k(0) = 1$, for $k\notin\mathcal{A}$ and $S_k(t)=0$ for $k\in\mathcal{A}$. The survival probability defines the probability that the system has not reached any absorbing configuration up to time $t$, given that it started at configuration $i$ at $t=0$. $J$ is the time dependent probability flux into the absorbing states, given that the system started at configuration $i$. 

The \textit{lifetime distribution function} is the sum over all absorbing states: $F_i(t):=\sum_{k\in\mathcal{A}}P_{ki}(t)$. From the lifetime distribution $F_i(t)$ one can find the probability that the system reached any absorbing configuration between time $t$ and $t+ dt$, as $F_i(t+dt)-F_i(t)=S_i(t)-S_i(t+dt)$. Therefore the first passage distribution can be found from 
\begin{equation*}
w_i(t)dt=\frac{dF_i(t)}{dt}dt =-\frac{dS_i(t)}{dt}dt
\end{equation*}

All $n$ moments of the first passage time are
\begin{equation*}
\left< T^n\right>=\int_0^\infty w_i(t)t^ndt
\end{equation*}
for which, the first moment, $n=1$, gives the mean first passage time 
\begin{equation*}
\left<T\right>=\int_0^\infty S_i(t)dt
\end{equation*}

For the continuous representation we now define $P(y_j,t|x_j,0)$ as the probability that all particles $j$ are located between $y_j$ and $y_j+dy_j$ at time $t$ given that they were at position $x_j$ at time $t=0$. Expanding the probability flux equation as a \href{http://en.wikipedia.org/wiki/Taylor_series}{Taylor series}, we reach the \href{http://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation}{Fokker-Planck equation} 
\begin{equation*}
\frac{\partial P(y_j,t|x_j,0)}{\partial t}=\sum_{k=1}^N\nabla_k\cdot(V_kP)+\sum_{k=1}^N\nabla_k^2(D(y_k)P)=\mathcal{L}P(y_j,t|x_j,0)
\end{equation*}
were $\nabla_k$ is the gradient with respect to the $k^{th}$ particle, $N$ is the total number of particles, and $\mathcal{L}$ is the Fokker Planck operator.
The density $P(y_j,t|x_j,0)$ obeys the backward Kolmogorov equation 
\begin{equation*}
\partial_tP(y_j,t|x_j,0)=\mathcal{L^\dagger}P(y_j,t|x_j,0)
\end{equation*} 
with $\mathcal{L^\dagger}=\sum_k^N V_k\cdot\nabla_k+\sum_k^ND(x_j)\nabla_k^2$ is the operator adjoint to $\mathcal{L}$.

$\mathcal{L^\dagger}$ operates on the initial position $x_j$, so we can integrate over the $y_j$ within the domain, excluding the absorbing surface and get the survival probability 
\begin{equation*}
\partial_tS(x_j;t)=\mathcal{L^\dagger}(S(x_j;t))
\end{equation*}
with the boundary conditions $S(x_j;t)=0,\forall x_j\in\partial\Omega_\mathcal{A}$, and $S(x_j;0)=1, \forall x_j\notin\partial\Omega_\mathcal{A}$. where $\partial \Omega_\mathcal{A}$ is the absorbing boundary. From the survival probability, all moments of the first passage time to an absorbing boundary $\partial \Omega_\mathcal{A}$ can be calculated. The following explicit relation holds 
\begin{equation*}
\mathcal{L^\dagger}\left<T^n(x_j)\right> = -n\left<T^{n-1}(x_j)\right>
\end{equation*}

\section{The Fokker-Planck equation}\label{section_theFokkerPlanckEquation}
For an \^{I}to process given by the stochastic equation, 
\begin{equation*}
dX_t=\mu(X_t,t)dt+\sqrt{2D(X_t,t)}dW_t
\end{equation*}
with drift $\mu(X_t,t)$ and diffusion $D(X_t,t)$, and a Weiner process $W_t$, the Fokker Planck-equation for the probability density $f(x,t)$ of $X_t$ is 
\begin{equation*}
\frac{\partial}{\partial t}f(x,t)=-\frac{\partial}{\partial x}[\mu(x,t)f(x,t)]+\frac{\partial^2}{\partial x^2}[D(x,t)f(x,t)]=\mathcal{L}f(x,t)
\end{equation*}
\subsection{Eigenvalues of the Fokker-Planck operator }\label{subsection_eigenvaluesOfTheFokkerPlanckOperator}
To find the eigenvalues of the Fokker Planck operator, we solve $\mathcal{L}f = \lambda f$.
rearranging the terms in this equation we arrive at 
\begin{equation*}
\frac{\partial^2}{\partial x^2}f(x,t) +p(x,t)\frac{\partial}{\partial x}f(x,t) +q(x,t)f(x,t) = 0
\end{equation*}
where 
\begin{equation*}
p(x,t) = \frac{2\frac{\partial}{\partial x}D(x,t)-\mu(x,t)}{D(x,t)}
\end{equation*}
\begin{equation*}
q(x,t)=\frac{\frac{\partial^2}{\partial x^2}D(x,t)-\lambda-\frac{\partial}{\partial x}\mu(x,t)}{D(x,t)}
\end{equation*}
which is a second order, homogeneous, ordinary differential equation with variable coefficients. 

\section{Diffusion on a sphere}
For a Diffusion process on a sphere we need to define a spherical diffusion propagator. One possibility is to define a spherical Gaussian, however this has the undesirable effect of 
% The bibliography
\bibliographystyle{plain}
\bibliography{stochasticProcessesBibliography} % the bibliography.bib file 
\end{document}