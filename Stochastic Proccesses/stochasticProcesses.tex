\documentclass[12pt]{report}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{float}

\begin{document}
\tableofcontents
\chapter{Basic Probability Concepts}
\section{$\sigma$-algebra, Probability space, and Random Variables}
From \cite{Oksendal14}. If $\Omega$ is a given set, a \textbf{$\sigma$-algebra} $\mathcal{F}$ on $\Omega$ is a family $\mathcal{F}$ of subsets of $\Omega$ with the following properties
\begin{enumerate}
\item $\emptyset \in \mathcal{F}$
\item for a set $F$, if $F\in \mathcal{F}$ then $F^C\in\mathcal{F}$, for $F^C= \Omega\backslash\mathcal{F}$
\item for sets $A_1,A_2,A_3,...\in\mathcal{F}\Rightarrow \cup_{i=1}^\infty A_i \in \mathcal{F}$
\end{enumerate}
Why is it important to have a $\sigma$-algebra in a probability space?
The $\sigma$-algebra represents all events that we can assign probabilities to. Property 1 together with property 2 states that in the space the probability of all events occurring, should be measurable and equal 1, whereas it's complement, no events occurred should be measurable and assigned probability 0. For other events, the probability of an event occurring and its complement should both be measurable. Property 3 assigns some "continuity" in the space of events and allows us to "group" elementary events without risking the creation of an event which is not included in the event space. 

The pair $(\Omega,\mathcal{F})$ is called a measurable space. A probability measure $P$ is a function $P:\mathcal{F}\rightarrow [0,1]$ which assigns each elementary event $A\in\mathcal{F}$ with a number representing its likelihood. 

The triplet $(\Omega,\mathcal{F},P)$ is a \textbf{probability space}. All probability spaces can be made complete, hence we refer to them as complete from now on. 

A function $Y:\Omega\rightarrow\mathbb{R}^n$ is said to be $\mathcal{F}$-measurable if
\begin{equation*}
Y^{-1}(U):=\{\omega\in \Omega ; Y(\omega)\in \mathcal{F} \}
\end{equation*}
For all open sets $U\in\mathbb{R}^n$. We call $\omega$ an elementary event. It is, in fact, a continuous trajectory in the space of continuous functions (but non-differentiable). 

A \textbf{random variable} $X$ is an $\mathcal{F}$-measurable function $X:\Omega\rightarrow \mathbb{R}^n$. Every random variable induces a probability measure $\mu_X$ on $\mathbb{R}^n$, defined by 

\begin{equation*}
\mu_X(B) = P(X^{-1}(B))
\end{equation*}

The function 
\begin{equation*}
(\omega,t)\rightarrow X(\omega,t)
\end{equation*}
represent a two-dimensional process, where fixing $t$ gives us all the values of the random process $X$ at a particular point in time (assuming $t$ represents time), whereas fixing $\omega$ gives us one trajectory over time (one realization of the process).The process $X$ is referred to as a \textbf{stochastic process}.


\section{The Space of Elementary Events}
From \cite{schuss2009theory}. The space of elementary events for a Brownian motion is the space of all continuous real functions 

\begin{equation*}
\Omega=\{\omega(t):\mathbb{R}_+\rightarrow\mathbb{R}\}
\end{equation*}


\section{Noise}[Unfinished]
White noise is a process for which there is no correlation between any values at different times. As an idealization of such a process there exist the \textbf{Orenstein-Uhlenbeck process} (to be defined) for which produces an almost uncorrelated noise. For this model, the second order correlation function can, up to a constant factor, be described as 
\begin{equation*}
<x(t),x(t')>= \frac{\gamma}{2}\exp(-\gamma|t-t'|)
\end{equation*}

\section{Cylinder Sets}
A cylinder set of Brownian trajectories is defined by a sequence of times $0\leq t_1<t_2<...<t_n$, and real intervals $I_k=(a_k,b_k)$, $k=1...n$, as 
\begin{equation*}
C(t_1,t_2,...,t_n;I_1,I_2,...I_n)=\{\omega\in \Omega|x(\omega,t_k)\in I_k\},\forall 1\leq k \leq n
\end{equation*}
For a cylinder not to contain a trajectory, it is enough that in one time point the trajectory is not in the interval.
The cylinder $C$ contains entire trajectory, which fulfill the inclusion demand. 

The joint PDF of $w(t_1,\omega),w(t_2,\omega),w(t_3,\omega)$ is the Weiner measure of the cylinder $C(t_1,t_2,t_3;I^x,I^y,I^z)$. These points belong to the same process $\omega$ at three different times. 

\section{Filtration}
The $\sigma$-algebra $\mathcal{F}_t$ of Brownian event is defined by cylinder
sets confined to times $0\leq t_i < t$, for some fixed $t$. Obviously, $\mathcal{F}_s \subset \mathcal{F}_t \subset \mathcal{F}$ if $0\leq s < t <\infty$. The family of $\sigma$âˆ’ algebras $\mathcal{F}_t$ for $t \geq 0$ is called the Brownian filtration and is said to be generated by the Brownian events up to time $t$

In simple words, filtration is an increasing sequence of $\sigma$-algebras on a measurable space. 

\section{Adapted Process}
The process $x(t,\omega)$ is said to be adapted
to the Brownian filtration $\mathcal{F}_t$ if $\{\omega\in\Omega|x(t,\omega) \leq y\} \in F_t$ for every $t \geq 0$ and
$y\in R$. In that case we also say that $x(t, \omega)$ is $\mathcal{F}_t$-measurable.

Thus an adapted process does not depend on the future behavior of the Brownian trajectory from time $t$ on.

\section{The Weiner Measure}\label{theWeinerMeasure}
The probability measure $Pr$ defined on all the Brownian trajectories (events in the set $\Omega$) are defined on cylinder sets and then extended to all events in the space by elementary properties of probability sets.

For a cylinder set $C(t;I)$, where $t>0$ and $I=(a,b)$, the Weiner measure is 
\begin{equation*}
Pr\{C(t;I)\}=\frac{1}{\sqrt{2\pi t}}\int_a^b\exp^{-x^2/2t}dx
\end{equation*}

\section{Martingale}\label{martingale}
A martingale is a stochastic process $ x(t)$ with $\mathbb{E}|x(t)|<\infty$ ,$\forall t$, and for $t_1<t_2<...<t_n$
\begin{equation*}
\mathbb{E}[x(t)|x(t_1)=x1,x(t_2)=x_2,...x(t_n)=x_n]=x_n
\end{equation*}


\section{Stochastic Integration}
For an arbitrary function of time $G(t')$ and a Weiner process $W(t)$, the stochastic integral 
\begin{equation*}
\int_{t_0}^t G(t')\mathrm{d}W(t')
\end{equation*}
is defined as a kind of Riemann integral, that is, we divide the interval $[t_0,t]$ into $n$ subintervals $t_0\leq t_1\leq ...\leq t$, and define intermediate points $t_{i-1}\leq\tau_i\leq t_{i}$. The integral is then defined as a limit of the partial sums 
\begin{equation*}
S_n=\sum_{i=1}^nG(\tau_i)[W(t_i)-W(t_{i-1})]
\end{equation*}
Note that the Weiner process in the partial sums is contributing the values at the edges of the interval, whereas the function $G$ contributes the values somewhere in its middle.
The choice of $\tau_i$ affects the value of the integral. It can be seen that f we choose $\tau_i=\alpha t_{i-1}+(1-\alpha)t_i$, with $0\leq \alpha\leq 1$,  than the mean value of the integral can take any value between 0 and $(t-t_0)$. 

for the \textbf{It\^{o} integral}, we choose $\tau_i=t_{i-1}$, which is the left boundary of the subinterval and thus define the stochastic integral as
\begin{equation*}
\int_{t_0}^{t}G(t')\mathrm{d}W(t')=ms-lim_{n\rightarrow\infty}\left\{\sum_{i=1}^nG(t_{i-1})[W(t_{i})-W(t_{i-1})] \right\}
\end{equation*}
by ms-lim, we mean the mean-square limit, which is defined by 
\begin{equation*}
ms-lim_{n\rightarrow\infty}\int p(\omega)[X_n(\omega)-X(\omega)]^2dw= lim_{n\rightarrow \infty}<(X_n-X)^2>
\end{equation*}

if indeed $lim_{n\rightarrow \infty}<(X_n-X)^2>$ we say that $ms-lim_{n\rightarrow \infty} X_n=X$.

One major consequence of the choice $\tau_i=t_{i-1}$ is that 
\begin{equation*}
\int_{t_0}^{t}W(t')dW(t')=0
\end{equation*}
That is, the stochastic integral of the Weiner process over any interval is zero.
Because the left point of the subinterval is chosen, the value of the function $G(t'_{i-1})$ is independent of the increments $[W(t_i)-W(t_{i-1})]$, since $G$ is $F_t$ adapted process.

\section{Properties of It\^{o} integral}\label{propertiesOfTheItoIntegral}
We define the class $H_2[0,T]$ of $F_t$-adapted stochastic processes such that 
\begin{equation*}
\int_0^T \mathbb{E}f^2(s,\omega)ds <\infty
\end{equation*}
and list the following properties
\begin{enumerate}
\item \textbf{linearity}. For $f(t),g(t)\in H_2[0,T]$, and $\alpha,\beta$ real, then $\alpha f(t)+\beta g(t) \in H_2[0,T]$, and 
\begin{equation*}
\int_0^t \alpha f(s)+\beta g(s) d\omega(s) = \alpha \int_0^t f(s)d\omega(s) +\beta \int_0^t g(s)d\omega(s)
\end{equation*}
\item \textbf{additivity}. If $f(t)\in H_2[0,T_1]$ and $f(t)\in H_2[T_1,T]$ for $0<T_1<T$, then 
\begin{equation*}
\int_0^Tf(s)d\omega(s) = \int_0^{T_1}f(s)d\omega(s) +\int_{T_1}^T f(s)d\omega(s)
\end{equation*}
\item for a deterministic and integrable $f(t)$
\begin{equation*}
\int_0^t f(s)d\omega(s) \sim N\left(0,\int_0^t f^2(s)ds\right)
\end{equation*}
\item for $f(t)\in H_2[0,T]$
\begin{equation*}
\mathbb{E}\int_0^t f(s)d\omega(s) = 0
\end{equation*}
\item for $0<\tau<t<T$  
\begin{equation*}
\mathbb{E}\left[\int_0^t f(s)d\omega(s)  | \int_0^\tau f(s)d\omega(s)=x \right] =x 
\end{equation*}
\item for $f(t),g(t)\in H_2[0,T]$
\begin{equation*}
\mathbb{E}\left[\int_0^Tf(s)d\omega(s)\int_0^T g(s)d\omega(s)\right]=\int_0^T\mathbb{E}[f(s)g(s)]ds
\end{equation*}
note that the right-hand-side is integrated with respect to $s$.
\end{enumerate}
We can see by property 5 that the It\^{o} integral is a martingale (see subsection \ref{martingale}).

\section{Integration with Respect to a Measure}
A measure $\mu_A$ defines an integral of a nonnegative measurable function $f(\omega)$ by 
\begin{equation*}
\int_{\Omega}f(\omega)d\mu(\omega)=\lim_{h\rightarrow 0}\lim_{N\rightarrow \infty}\sum_{n=0}^{N}nh\mu\{\omega: nh\leq f(\omega)\leq(n+1)h\}
\end{equation*}
If the limit exists we say the $f(\omega)$ is a measurable function. 


\section{The It\^{o} Calculus}
From \cite{schuss2009theory} p.82. Consider two processes $a(t),b(t)$ of class $H_2[0,T]$ (defined in \ref{propertiesOfTheItoIntegral}) and define the stochastic process
\begin{equation*}
x(t) = x_0 + \int_0^t a(s)ds + \int_0^t b(s)d\omega(s)
\end{equation*}

where $x_0$ is a random variable independent on $\omega(t),\forall t>0$, then for $0\leq t_1\leq t_2 \leq T$
\begin{equation*}
x(t_2)-x(t_1)=\int_{t_1}^{t_2}x(s)ds +\int_{t_1}^{t_2}b(s)d\omega(s)
\end{equation*}
We abbreviate the this notation by 
\begin{equation*}
dx=a(t)dt+b(t)d\omega(t)
\end{equation*}

\textbf{Example 1:} We calculate the differential of $\omega^2(t)$. By the definition of the Ito integral we know that 
\begin{equation*}
\omega^2(t_2)-\omega^2(t_1)=(t_2-t_1)+2\int_{t_1}^{t_2}\omega(s)d\omega(s)=\int_{t_1}^{t_2}1dt+2\int_{t_1}^{t_2}\omega(s)d\omega(s)
\end{equation*}. 
In our abbreviation we can write 
\begin{equation*}
d\omega^2(t) = 1dt+2\omega(t)d\omega(t)
\end{equation*}
with $a(t)=1$ and $b(t)=2\omega(t)$. So the Ito calculus does not obey the normal chain rule of calculus.

\textbf{Example 2:} We now calculate the differential of $f(t)\omega(t)$. If $f(t)$ is a deterministic smooth function then we can integrate by parts
\begin{equation*}
\int_{t_1}^{t_2}f(t)d\omega(t)= f(t_2)d\omega(t_2)-f(t_1)d\omega(t_1)-\int_{t_1}^{t_2}f'(t)\omega(t)dt
\end{equation*}
we now set $x(t)= f(t)\omega(t)$ so 
\begin{equation*}
dx(t) = f'(t)d\omega(t)dt+f(t)d\omega(t)=\omega(t)df(t)+f(t)d\omega(t)
\end{equation*}
as in the classical calculus. 




\section{The Langevin Equation}{[Unfinished]}
An equation of the type 
\begin{equation*}
\frac{dx}{dt}=a(x,t)+b(x,t)\xi(t)
\end{equation*}

\section{The It\^{o} Equation}
Equation of the form 
\begin{equation*}
 \mathrm{d} X_t = \mu(X_t,t)\, \mathrm{d} t +  \sigma(X_t,t)\, \mathrm{d} B_t
\end{equation*}
is an informal way of expressing the more appropriate integral equation 
\begin{equation*}
X_{t+s} - X_{t} = \int_t^{t+s} \mu(X_u,u) \mathrm{d} u + \int_t^{t+s} \sigma(X_u,u)\, \mathrm{d} B_u
\end{equation*}
where the dynamic of the particle $X$ is given as a sum of two integrals, the first one is a standard Lebesgue integral, and the second is an It\^{o} integral (to be defined).  
The functions $\mu(X_t,t)$ and $\sigma(X_t,t)$ are well defined continuous functions, and $B_t$ is a Brownian motion (a Weiner process, or white noise). An informal way of interpreting the equation above is to say that at each small time interval of size $\delta$ the process $X_t$ is changed by a normally distributed value with expectation $\mu(X_t,t)\delta$ and variance $\sigma(X_t,t)^2\delta$. 
the function $\mu(X_t,t)$ is referred to a s the drift coefficient, and the function $\sigma(X_t,t)$ as the diffusion coefficient. 

\section{Existence and Uniqueness of It\^{o} SDE solutions}
For an Ito SDE taking values in n-dimensional Euclidean space, if for $T>0$\\
\begin{eqnarray*}
&\mu:&\mathbb{R}^2\times[0,T]\rightarrow \mathbb{R}^n\\
&\sigma:&\mathbb{R}^n \times[0,T] \rightarrow \mathbb{R}^{n\times m}
\end{eqnarray*}
are measurable functions, for which there exist constants $C$ and $D$ such that 
\begin{eqnarray*}
&&|\mu(x,t)|+|\sigma(x,t)|\leq C(1+|x|)\\
&&|\mu(x,t)-\mu(y,t)|+|\sigma(x,t)-\sigma(y,t)|\leq D|x-y|
\end{eqnarray*}
for all $t\in [0,T]$ and all $x,y\in \mathbb{R}^n$, and $|\sigma|^2=\sum_{i,j=1}|\sigma_{i,j}|^2$. 
If $Z$ is a random variable, independent of the $\sigma$-algebra generated by $B_s$, $s\geq 0$, and with finite second moment, then the SDE, with initial condition $X_0=Z$ has an almost surely unique solution in $t\in[0,T]$, $X_t(\omega)$, such that $X$ is adapted to the filtration $\mathcal{F}_t$ generated by $Z$ and $B_s$, $s\leq t$.

\section{Non-anticipating Functions}{[Unfinished]}

\section{Stochastic Differential Equations}



% The bibliography
\bibliographystyle{plain}
\bibliography{stochasticProcessesBibliography} % the bibliography.bib file 
\end{document}